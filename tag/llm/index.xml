<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM | Jiwen Jiang</title>
    <link>https://jiwenj.github.io/tag/llm/</link>
      <atom:link href="https://jiwenj.github.io/tag/llm/index.xml" rel="self" type="application/rss+xml" />
    <description>LLM</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 19 Oct 2023 21:10:00 +0700</lastBuildDate>
    <image>
      <url>https://jiwenj.github.io/media/icon_hu7ee7877cac8e8b3a44b2d944c7dd79b2_197985_512x512_fill_lanczos_center_3.png</url>
      <title>LLM</title>
      <link>https://jiwenj.github.io/tag/llm/</link>
    </image>
    
    <item>
      <title>CUDA Programming</title>
      <link>https://jiwenj.github.io/blog/gpu/</link>
      <pubDate>Thu, 19 Oct 2023 21:10:00 +0700</pubDate>
      <guid>https://jiwenj.github.io/blog/gpu/</guid>
      <description>&lt;hr&gt;
&lt;para/&gt;
&lt;ul&gt;
&lt;li&gt;Table
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#books&#34;&gt;Books&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#courses&#34;&gt;Courses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#blogs&#34;&gt;Blogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#faculty&#34;&gt;Faculty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;para&gt;
&lt;h3 id=&#34;books&#34;&gt;Books&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;对应中文版《GPU高性能编程CUDA实战》《CUDA并行程序设计：GPU编程指南​》《CUDA专家手册 : GPU编程权威指南》《大规模并行处理器编程实战（第2版）​》《CUDA编程指南4.0中文版》&lt;/li&gt;
&lt;li&gt;CUDA C编程权威指南》 《GPU编程实战（基于Python和CUDA）》书《GPU编程与优化•大众高性能计算》&lt;/li&gt;
&lt;li&gt;CUDA 编程: 基础与实践 &lt;a href=&#34;https://github.com/JiwenJ/Public-Books/blob/main/cuda/CUDA%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%AE%9E%E8%B7%B5%20%28%E6%A8%8A%E5%93%B2%E5%8B%87%29%20%28Z-Library%29.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cook, Shane. CUDA 并行程序设计 – GPU 编程指南, 机械工业出版社, 2014.&lt;/li&gt;
&lt;li&gt;AI 编译器开发指南&lt;/li&gt;
&lt;li&gt;Programming Massively Parallel Processors: A Hands-on Approach - CUDA by Example: An Introduction to General-Purpose GPU Programming》&lt;/li&gt;
&lt;li&gt;CUDA by ExampleCUDA Programming: A Developer&amp;rsquo;s Guide to Parallel Computing with GPUs&lt;/li&gt;
&lt;li&gt;CUDA Programming Guide
​Programming Massively Parallel Processors: A Hands-on Approach
《大规模并行处理器编程实战（第2版）​》，英文版已经第4版，在 ScienceDirect 可下载(疫情期间上海高校好像可免费下载)。&lt;/li&gt;
&lt;li&gt;​CUDA Handbook: A Comprehensive Guide to GPU Programming&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/piDack/The-ans-for-Programming-Massively-Parallel-Processor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/piDack/The-ans-for-Programming-Massively-Parallel-Processor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The CUDA HandbookProgramming Massively Parallel Processors 2rd&lt;/li&gt;
&lt;li&gt;CUDA by Example: An Introduction to General-Purpose GPU Programming
Programming Massively Parallel Processors 3th- Nvidia Cuda Programming Guide&lt;/li&gt;
&lt;li&gt;Programming Massively Parallel Processors&lt;/li&gt;
&lt;li&gt;Professional CUDA C Programming&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;footnote: Parts of the books can be found &lt;a href=&#34;https://github.com/JiwenJ/Public-Books/tree/main/cuda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;courses&#34;&gt;Courses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;udacity cs344: intro to parallel programming&lt;/li&gt;
&lt;li&gt;龚敏敏老师在哔哩哔哩上的《上帝视角看GPU》&lt;/li&gt;
&lt;li&gt;UIUC的课 Heterogeneous Parallel Programming&lt;/li&gt;
&lt;li&gt;CS179:GPU Programming&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-parallel-programming--cs344&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Udacity的《入门并行编程：CUDA》课程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;documentation&#34;&gt;Documentation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/cuda/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIDIA 官方文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-c-programming-guide/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIDIA 官方编程文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;blogs&#34;&gt;Blogs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;高升博客&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;community&#34;&gt;Community&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://devtalk.nvidia.com/default/board/57/cuda-programming-and-performance/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CUDA开发者社区&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;faculty&#34;&gt;Faculty&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;UC Davis John Owens，Billy Dally的学生&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;misc&#34;&gt;Misc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;OpenGL(ES) OpenCL Vulkan Taichi ROCm&lt;/li&gt;
&lt;li&gt;产品datasheet各个架构的白皮书&lt;/li&gt;
&lt;li&gt;cutlass的discussion，&lt;/li&gt;
&lt;li&gt;tensorcore资料又不是很多（市面上往往还是调用nvcuda::wmma这个封装好的api&lt;/li&gt;
&lt;li&gt;他们也有discord频道，&lt;/li&gt;
&lt;li&gt;reduce, &lt;a href=&#34;https://zhuanlan.zhihu.com/p/441146275&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;矩阵乘&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comment&#34;&gt;Comment:&lt;/h3&gt;
&lt;p&gt;综上所述，如果只想入门、想了解众核并行、想锻炼C++工程能力，随便找本不错的书读；要是想保持在前沿，看博客、看文章、直接关注做GPU的组，比读几百页书知识还停留在10年前强多了&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/26570985&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.zhihu.com/question/26570985&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;para/&gt;
&lt;style&gt;
red { color: red; font-style:italic; font-family:Georgia; font-size:10px; font-weight:normal}
yellow { color: yellow }
para{ color: black; font-style:normal; font-family:Georgia; font-weight:normal}
&lt;/style&gt;
</description>
    </item>
    
    <item>
      <title>Inference tools for LLMs</title>
      <link>https://jiwenj.github.io/blog/inference/</link>
      <pubDate>Sat, 14 Oct 2023 21:10:00 +0700</pubDate>
      <guid>https://jiwenj.github.io/blog/inference/</guid>
      <description>&lt;!-- &amp;ensp;
&amp;emsp;
&amp;nbsp; --&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;Table
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#and-a-table-of-contents&#34;&gt;And a table of contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#on-the-right&#34;&gt;On   the right&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#l&#34;&gt;Use the [TOC]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt; “Garbage in, garbage out” is a classic saying in computing about how problematic input data or instructions will produce problematic outputs.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;para&gt; Before we do any training, the data we are going to feed into the LLM really matter.&lt;/para&gt;&lt;/p&gt;
&lt;h2 id=&#34;l&#34;&gt;l&lt;/h2&gt;
&lt;h5 id=&#34;reference&#34;&gt;Reference&lt;/h5&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://xueshu.baidu.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&#34;https://en.wikipedia.org/wiki/Main_Page&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;
red { color: red; font-style:italic; font-family:Georgia; font-size:10px; font-weight:normal}
yellow { color: yellow }
para{ color: red; font-style:normal; font-family:Georgia; font-weight:normal}
refer{}
&lt;/style&gt;
</description>
    </item>
    
    <item>
      <title>LLMs for Code (Progressing)</title>
      <link>https://jiwenj.github.io/blog/llm4code/</link>
      <pubDate>Sat, 14 Oct 2023 21:10:00 +0700</pubDate>
      <guid>https://jiwenj.github.io/blog/llm4code/</guid>
      <description>&lt;p&gt;&lt;para&gt;Large Language Models for Code are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. In this article, I curate some SOTA models and try to figure out their machanism. Here is the content of table:&lt;/para&gt;&lt;/p&gt;
&lt;!-- https://analyticsindiamag.com/metas-code-llama-is-here-but-unnaturally/ --&gt;
&lt;hr&gt;
&lt;para&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contents&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Datasets
&lt;ul&gt;
&lt;li&gt;HumanEval&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Models
&lt;ul&gt;
&lt;li&gt;CodeLlama&lt;/li&gt;
&lt;li&gt;Phind&lt;/li&gt;
&lt;li&gt;CodeGeex&lt;/li&gt;
&lt;li&gt;Codeium&lt;/li&gt;
&lt;li&gt;WizardCode&lt;/li&gt;
&lt;li&gt;phi-1&lt;/li&gt;
&lt;li&gt;CodeFuse&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/para&gt;
&lt;hr&gt;
&lt;p&gt;CodeXGLUE &lt;a href=&#34;https://paperswithcode.com/dataset/codexglue&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://paperswithcode.com/dataset/codexglue&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;humaneval&#34;&gt;HumanEval&lt;/h3&gt;
&lt;p&gt;&lt;para&gt;This is an evaluation harness for the HumanEval problem solving dataset described in the paper &lt;a href=&#34;https://arxiv.org/pdf/2107.03374.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Evaluating Large Language Models Trained on Code&amp;rdquo;&lt;/a&gt;. It used to measure functional correctness for synthesizing programs from docstrings. It consists of 164 original programming problems, assessing language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions. At the same time, you can find the latest models and advancements in &lt;a href=&#34;https://paperswithcode.com/sota/code-generation-on-humaneval&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PapersWithCode&lt;/a&gt;.&lt;/para&gt;
















&lt;figure  id=&#34;figure-an-example-the-white-space-is-the-prompt-or-input-while-the-yellow-space-is-the-model-generated-content&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./1.jpg&#34; alt=&#34;example&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An example: the white space is the prompt or input while the yellow space is the model-generated content
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;codellama&#34;&gt;CodeLlama&lt;/h3&gt;
&lt;p&gt;&lt;para&gt;Maybe you haven&amp;rsquo;t excperienced CodeLlama yet, the Huggingface official group has deployed a &lt;a href=&#34;https://huggingface.co/chat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;playground&lt;/a&gt; for everyone where you can Preliminarily evaluate it in your target task.&lt;/para&gt;&lt;/p&gt;
&lt;p&gt;&lt;para&gt;After you have seen its performance on your task and gain much confidence in CodeLlama, you might want to give it a shot where you can delve deeper into the paper, even deploy and finetune it. The model weights are available in &lt;a href=&#34;https://huggingface.co/codellama&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huggingface&lt;/a&gt;. If you go through the &lt;a href=&#34;https://arxiv.org/pdf/2308.12950v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;, you might find a really distinct model named &amp;ldquo;&lt;strong&gt;unnatural CodeLlama&lt;/strong&gt;&amp;rdquo; whose performance is nearly the same as GPT-4 under the code scenario.&lt;/para&gt;&lt;/p&gt;
&lt;h5 id=&#34;what-is-unnatural-instruction&#34;&gt;What is unnatural instruction&lt;/h5&gt;
&lt;p&gt;&lt;para&gt;In December 2022, Meta AI with Tel Aviv University published the paper named &lt;a href=&#34;https://arxiv.org/pdf/2212.09689.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor&lt;/a&gt;. The paper talks about how Meta created a large dataset of creative and diverse instructions and collected 64,000 examples by prompting a language model. This was then further prompted to create a total of 240,000 examples on inputs and outputs, which contained only a little amount of noise. Unnatural Instructions is a dataset of instructions automatically generated by a Large Language model.&lt;/para&gt;&lt;/p&gt;
&lt;p&gt;&lt;para&gt;An example of &lt;a href=&#34;https://github.com/orhonovich/unnatural-instructions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dataset&lt;/a&gt;:&lt;/para&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-txt&#34; data-lang=&#34;txt&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &amp;#39;instruction&amp;#39;: &amp;#39;You will be given a series of words. Output these words in reverse order, with each word on its own line.&amp;#39;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &amp;#39;instances&amp;#39;:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  [{
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#39;instruction_with_input&amp;#39;: &amp;#34;You will be given a series of words. Output these words in reverse order, with each word on its own line.\nWords: [&amp;#39;Hello&amp;#39;, &amp;#39;world&amp;#39;].&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#39;input&amp;#39;: &amp;#34;Words: [&amp;#39;Hello&amp;#39;, &amp;#39;world&amp;#39;].&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#39;constraints&amp;#39;: &amp;#39;None.&amp;#39;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#39;output&amp;#39;: &amp;#39;world\nHello&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  }]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;codeium&#34;&gt;Codeium&lt;/h3&gt;
&lt;h3 id=&#34;codegeex2&#34;&gt;CodeGeeX2&lt;/h3&gt;
&lt;p&gt;&lt;para/&gt;CodeGeeX2 是多语言代码生成模型 CodeGeeX (KDD’23) 的第二代模型。不同于一代 CodeGeeX（完全在国产华为昇腾芯片平台训练） ，CodeGeeX2 是基于 ChatGLM2 架构加入代码预训练实现，得益于 ChatGLM2 的更优性能，CodeGeeX2 在多项指标上取得性能提升（+107% &amp;gt; CodeGeeX；仅60亿参数即超过150亿参数的 StarCoder-15B 近10%）&lt;/p&gt;
&lt;h3 id=&#34;wizardcode&#34;&gt;WizardCode&lt;/h3&gt;
&lt;p&gt;&lt;para/&gt;Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic&amp;rsquo;s Claude and Google&amp;rsquo;s Bard, on HumanEval and HumanEval+.&lt;/p&gt;
&lt;h3 id=&#34;phi-1&#34;&gt;phi-1&lt;/h3&gt;
&lt;p&gt;&lt;para&gt;phi-1 is proposed by teams of Microsoft in the &lt;a href=&#34;https://arxiv.org/pdf/2306.11644v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;quot;&lt;em&gt;Textbooks Are All You Need&lt;/em&gt;&amp;quot;&lt;/a&gt;. phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of &amp;ldquo;textbook quality&amp;rdquo; data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.
&lt;/para&gt;&lt;/p&gt;
&lt;h3 id=&#34;phind&#34;&gt;Phind&lt;/h3&gt;
&lt;p&gt;&lt;para/&gt;&lt;a href=&#34;https://www.phind.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Phind&lt;/a&gt; is an intelligent search engine and assistant for programmers. With Phind, you&amp;rsquo;ll get the answer you&amp;rsquo;re looking for in seconds instead of hours. It will guide you step-by-step from that idea in your head to a working app. Phind is smart enough to proactively ask you questions to clarify its assumptions and to browse the web (or your codebase) when it needs additional context. With their new VS Code extension, you can now get Phind&amp;rsquo;s help right in your editor.&lt;/p&gt;
&lt;p&gt;&lt;para/&gt;August 28th Update: They&amp;rsquo;ve trained a new model, Phind-CodeLlama-34B-v2, that achieves 73.8% pass@1 on HumanEval. See the &lt;a href=&#34;https://www.phind.com/blog/code-llama-beats-gpt4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;para/&gt;They have released all models on &lt;a href=&#34;https://huggingface.co/Phind&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huggingface&lt;/a&gt; to bolster the open-source community.&lt;/p&gt;
&lt;h3 id=&#34;vscode-plugin-with-llms&#34;&gt;Vscode Plugin with LLMs&lt;/h3&gt;
&lt;p&gt;&lt;para&gt;We have mentioned above that most of the models are incorporated into the plugin for the convenience in coding procedure. In order to find out the mechanism, we take CodeGeex as an example. First we install the CodeGeex plugin and log in, then we can use &lt;code&gt;developer tool (ctl+shift+I)&lt;/code&gt; to ananlyze the network and console output.
We further go to the directory &lt;code&gt;&#39;~/.vscode/aminer.codegeex-2.1.3&#39;&lt;/code&gt; where exists the source code of the plugin and find the post method in line42-64 of &lt;code&gt;&#39;~/.vscode/extensions/aminer.codegeex-2.1.3/out/utils/getCodeCompletions.js&#39;&lt;/code&gt;. Besides, with seamless effort we can obtain the api url: &lt;code&gt;&#39;https://tianqi.aminer.cn/api/v2/multilingual_code_generate_adapt&#39;&lt;/code&gt;, which you can try with any frontend tools. So far we can get preliminary thought about the mechanism: Frontend + Api calling, so the keys of user experience are still the &lt;code&gt;&#39;clever&#39;&lt;/code&gt; model and network.&lt;/para&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;para&gt;In the conclusion, I will display the performance of these models on HumanEval dataset so that we can clearly and conveniently see their gap in code generation task. Therefore we can choose the right base model for our further research and production.&lt;/para&gt;&lt;/p&gt;
&lt;table align=&#34;center&#34; style=&#34;width:100%;border: solid;border-width:2px 0&#34;&gt;
&lt;caption&gt;Table 1: Comparable results on code generation task&lt;/caption&gt;
&lt;thead style=&#34;border-bottom:#000 2px solid;&#34;&gt;
&lt;tr&gt;
&lt;th style=&#34;border:0&#34;&gt;Model&lt;/th&gt;
&lt;td style=&#34;border:0&#34;&gt;Size&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;Open source&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;Techniques&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;Pass@1&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;Pass@10&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;Pass@100&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;Plugin&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;thead&gt;
&lt;tr &gt;
&lt;th style=&#34;border:0&#34;&gt;CodeLlama&lt;/th&gt;
&lt;td style=&#34;border:0&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;border:0&#34; &gt;&amp;#10004&lt;/td&gt;
&lt;td style=&#34;border:0&#34; &gt;&amp;#10008&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border:0&#34;&gt;WizardCode&lt;/th&gt;
&lt;td style=&#34;border:0&#34;&gt;F&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border:0&#34;&gt;Phind&lt;/th&gt;
&lt;td style=&#34;border:0&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border:0&#34;&gt;Codeium&lt;/th&gt;
&lt;td style=&#34;border:0&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border:0&#34;&gt;Phi&lt;/th&gt;
&lt;td style=&#34;border:0&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;border:0&#34;&gt;CodeGeex&lt;/th&gt;
&lt;td style=&#34;border:0&#34;&gt;M&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;border:0&#34;&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;style&gt;
red { color: red; font-style:italic; font-family:Georgia; font-size:10px; font-weight:normal}
yellow { color: yellow }
para{ color: black; font-style:normal; font-family:Georgia; font-weight:normal}

th,td {
    text-align: center;
    font-family:Georgia
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Supervised Pretraining Can Learn In-Context Reinforcement Learning</title>
      <link>https://jiwenj.github.io/paperreview/1/</link>
      <pubDate>Mon, 17 Jul 2023 18:00:47 +0800</pubDate>
      <guid>https://jiwenj.github.io/paperreview/1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Prompt Engineering for LLMs</title>
      <link>https://jiwenj.github.io/blog/prompt-engineering/</link>
      <pubDate>Mon, 05 Jun 2023 21:10:00 +0700</pubDate>
      <guid>https://jiwenj.github.io/blog/prompt-engineering/</guid>
      <description>&lt;h5 id=&#34;group-meeting-pptpptpromptpdf&#34;&gt;&lt;a href=&#34;https://jiwenj.github.io/ppt/prompt.pdf&#34;&gt;Group Meeting PPT&lt;/a&gt;&lt;/h5&gt;
</description>
    </item>
    
  </channel>
</rss>
